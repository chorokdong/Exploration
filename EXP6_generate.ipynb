{"cells":[{"cell_type":"markdown","metadata":{"id":"uq1vZGOjb2p7"},"source":["# 1. 필요 라이브러리 불러오기"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4369,"status":"ok","timestamp":1652787331909,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"WK0Wmexwbxq4"},"outputs":[],"source":["import glob\n","import tensorflow as tf\n","import os, re\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import keras"]},{"cell_type":"markdown","metadata":{"id":"BUUz1hfvb5rW"},"source":["# 2. 데이터 가져오기"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1652787331910,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"f65F2NFAb2WM","outputId":"2c4c13ac-cb29-40c5-9b32-cfd863ec7cf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["데이터 크기: 187088\n","Examples:\n"," [\"Let's stay together I, I'm I'm so in love with you\", 'Whatever you want to do', 'Is all right with me']\n"]}],"source":["txt_file_path = '/content/drive/MyDrive/aiffel/Exp6/data/EXP6/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"]},{"cell_type":"markdown","metadata":{"id":"-_YkU-8weP_s"},"source":["# 3. 데이터 정제"]},{"cell_type":"markdown","metadata":{"id":"bsIg9sMvgLuS"},"source":["위에서 확인 결과 공백 또는 문장끝이 : 인 문장은 없지만 혹시 모르니 처리하주겠다."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1652787331910,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"Zj4847wld5XE","outputId":"398ed305-8710-4b6c-f5bc-e9f86a4b0920"},"outputs":[{"name":"stdout","output_type":"stream","text":["Let's stay together I, I'm I'm so in love with you\n","Whatever you want to do\n","Is all right with me\n","Cause you make me feel so brand new\n","And I want to spend my life with you Let me say that since, baby, since we've been together\n","Loving you forever\n","Is what I need\n","Let me, be the one you come running to\n","I'll never be untrue Oh baby\n","Let's, let's stay together (gether)\n"]}],"source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n","\n","    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","        \n","    print(sentence)"]},{"cell_type":"markdown","metadata":{"id":"l9k2rUWaj1j7"},"source":["#### 변경할 내용\n","- 대문자 -> 소문자\n","- 특수문자 [?.!,¿] 제외하고 제거\n","- 특수문자 [']도 i'ii / let's 등 때문에 제외\n","- 문장 앞부분과 끝부분에 <start>, <end> 추가"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1652787331910,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"yZ3BP10VfCof"},"outputs":[],"source":["# sub(패턴,교체할문자열,문자열,최대교체수) : 패턴에 맞으면 그 문자를 교체할문자열로 교체는 함수\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2. 특수문자 양쪽에 공백을 넣고\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", sentence) # 4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다 + let's / i'll/ i'm과 같은 문자열을 위해 ' 도 추가\n","    sentence = sentence.strip() # 5. 다시 양쪽 공백을 지웁니다\n","    sentence = '<start> ' + sentence + ' <end>' # 6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","    return sentence"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2380,"status":"ok","timestamp":1652787334763,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"kVqS3QvJhFNk","outputId":"3dd8ca09-70dc-4e22-f80d-72a663e89894"},"outputs":[{"data":{"text/plain":["[\"<start> let's stay together i , i'm i'm so in love with you <end>\",\n"," '<start> whatever you want to do <end>',\n"," '<start> is all right with me <end>',\n"," '<start> cause you make me feel so brand new <end>',\n"," \"<start> and i want to spend my life with you let me say that since , baby , since we've been together <end>\",\n"," '<start> loving you forever <end>',\n"," '<start> is what i need <end>',\n"," '<start> let me , be the one you come running to <end>',\n"," \"<start> i'll never be untrue oh baby <end>\",\n"," \"<start> let's , let's stay together gether <end>\"]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","\n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"]},{"cell_type":"markdown","metadata":{"id":"iv4wVobYlNA4"},"source":["#### 예제에서는 토큰수 15개 이상일 경우 제외를 권장하였다. 해당 글을 읽고 제외하는 방법이 2가지가 있다고 생각이 들어 하나씩 실험을 해보려 한다.\n","\n","- 첫 번째는, 토큰화한 후 start와 end를 포함한 길이가 17개가 넘어서는 토큰은 모두 제외한다.\n","\n","- 두 번째, maxlen을 17로 설정한 뒤 작문과 val_loss값을 확인한다.\n","\n","- 세 번째, maxlen을 25로 설정한 뒤 작문과 val_loss값을 확인한다.\n","\n","- 첫번째 방법을 사용하면 전체 데이터셋에서 13000여개가 제외되나, 두번째, 세번째 방법을 사용하면 데이터셋의 수는 동일하게 유지된다."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":447,"status":"ok","timestamp":1652787335194,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"zDU5qPmH-oc5","outputId":"4ee0d56b-d209-4a4e-8191-9dd6b1705f42"},"outputs":[{"name":"stderr","output_type":"stream","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}],"source":["# <start>와 <end>를 포함하여 17개 이상의 문장은 제외\n","corpus15 = []\n","for i in range(len(corpus)):\n","  if len(corpus[i].split()) <= 17:\n","    corpus15.append(corpus[i])\n","print(corpus15)\n","  "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1652787335195,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"4R6ABXiiGJ_Q","outputId":"180d7969-1e1c-4da0-82be-15ce656d5cdf"},"outputs":[{"data":{"text/plain":["165330"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# 제외된 데이터셋 크기 확인\n","len(corpus15)"]},{"cell_type":"markdown","metadata":{"id":"uSjcuEPPrakL"},"source":["#### pre_sequences 파라미터\n","sequences: 리스트의 리스트로, 각 성분이 시퀀스이다       \n","maxlen: 정수, 모든 시퀀스의 최대 길이를 설정하여 제한한다. 10을 넣으면 10보다 큰 것들은 자른다.      \n","dtype: 출력 시퀀스의 자료형. 가변적 길이의 문자열로 시퀀스를 패딩 하려면object를 사용하시면 됩니다.      \n","padding: 문자열이 들어간다, 'pre'가 디폴트 값으로 앞쪽에 0이 추가되고, 'post'는 뒤쪽으로 0이 추가되어 각 시퀀스를 패딩 한다.      \n","truncating: 문자열, 'pre'는 길이가 초과됐을 때 앞쪽을 자르고 'post'는 maxlen보다 큰 시퀀스의 끝의 값들을 제거한다      \n","value: 부동소수점 혹은 문자열, 패딩 할 값.      "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3854,"status":"ok","timestamp":1652787339046,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"Z4nESCztCzE4","outputId":"438d22bf-e8a9-4317-c095-4f5952850cc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[   0    0    0 ...   28    7    3]\n"," [   0    0    0 ...   10   44    3]\n"," [   0    0    0 ...   28   11    3]\n"," ...\n"," [   0    0    0 ...   23 3462    3]\n"," [   0    0    0 ...  593  832    3]\n"," [   0    0    0 ... 2627  214    3]] <keras_preprocessing.text.Tokenizer object at 0x7f2aed54d550>\n"]}],"source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","\n","def tokenize(corpus15):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=12000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus15)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus15)   \n","\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus15)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1652787339047,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"EAhuokb3sAio","outputId":"be629dda-97a5-4e8d-9bc7-ff7c21a05bf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : the\n","6 : i\n","7 : you\n","8 : and\n","9 : a\n","10 : to\n"]}],"source":["# 단어 사전이 어떻게 구축 되어 있는지 확인\n","for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break\n","#<start>가 2번 이기 때문에 모든 행의 시작은 2였다."]},{"cell_type":"markdown","metadata":{"id":"i4oXzROJ7Fdl"},"source":["# 4. 데이터셋 분리"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1652787339047,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"nqF_yU4w5BVa"},"outputs":[],"source":["# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]  "]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1652787339047,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"v6GhnhK02M7v"},"outputs":[],"source":["enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state=124, shuffle=tgt_input)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1652787339047,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"5VO56vHb8sEf","outputId":"632759b5-2c87-4da5-fa95-6ca4e0d5de2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(132264, 16)\n","(33066, 16)\n","(132264, 16)\n","(33066, 16)\n"]}],"source":["print(enc_train.shape)\n","print(enc_val.shape)\n","print(dec_train.shape)\n","print(dec_val.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"kshCSb557Lvr"},"source":["# 5. 인공지능 만들기"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":279,"status":"ok","timestamp":1652789337867,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"ErTd2KDT7CtF"},"outputs":[],"source":["BUFFER_SIZE = len(enc_train) # 완벽한 셔플링을 위해서는 전체 데이터 세트의 크기보다 크거나 같은 버퍼 크기가 필요\n","BATCH_SIZE = 256\n","steps_per_epoch = len(enc_train) // BATCH_SIZE\n","\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1652789338272,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"bmiPe-j9W0Qt"},"outputs":[],"source":["BUFFER_SIZE = len(enc_val) # 완벽한 셔플링을 위해서는 전체 데이터 세트의 크기보다 크거나 같은 버퍼 크기가 필요\n","BATCH_SIZE = 256\n","steps_per_epoch = len(enc_val) // BATCH_SIZE\n","\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","\n","dataset_test = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n","dataset_test = dataset_test.shuffle(BUFFER_SIZE)\n","dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=True)\n","\n"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1652789646377,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"IMJv2TMjloUN"},"outputs":[],"source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    while True:\n","     \n","        predict = model(test_tensor) \n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":449,"status":"ok","timestamp":1652789406852,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"N_FFVxj87GEz"},"outputs":[],"source":["# epoch 마다 결과를 볼 수 있게끔 클래스 생성\n","class CustomCallback(keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch,logs=None):\n","      print(generate_text(model3, tokenizer3, init_sentence=\"<start> i\"))"]},{"cell_type":"markdown","metadata":{"id":"RSFjgldm8Qv5"},"source":["# 6. 인공지능 학습하기\n","\n","## 6-1 토큰의 길이가 17개를 넘어서는 문장 제외"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":259,"status":"ok","timestamp":1652789433235,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"eWj5N6vv7cKe"},"outputs":[],"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)\n","        self.rnn_3 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.2)\n","        self.linear2 = tf.keras.layers.Dense(5120)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.rnn_3(out)\n","        out = self.linear2(out)\n","        out = self.linear(out)\n","        return out\n","    \n","embedding_size = 128\n","hidden_size = 512\n","model2 = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147272,"status":"ok","timestamp":1652789584834,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"-H1GYPo17mi3","outputId":"aee24f22-d82c-423d-c115-4b0e0ec9d704"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/7\n","  5/516 [..............................] - ETA: 18s - loss: 2.7090WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0085s vs `on_train_batch_end` time: 0.0232s). Check your callbacks.\n","515/516 [============================>.] - ETA: 0s - loss: 2.6772<start> i don't want to be no more <end> \n","516/516 [==============================] - 26s 42ms/step - loss: 2.6769 - val_loss: 2.6608\n","Epoch 2/7\n","515/516 [============================>.] - ETA: 0s - loss: 2.6077<start> i don't want to be no more <end> \n","516/516 [==============================] - 20s 39ms/step - loss: 2.6077 - val_loss: 2.6213\n","Epoch 3/7\n","515/516 [============================>.] - ETA: 0s - loss: 2.5467<start> i don't want to be no more <end> \n","516/516 [==============================] - 20s 39ms/step - loss: 2.5465 - val_loss: 2.5897\n","Epoch 4/7\n","515/516 [============================>.] - ETA: 0s - loss: 2.5008<start> i don't want to be no more <end> \n","516/516 [==============================] - 20s 39ms/step - loss: 2.5008 - val_loss: 2.5628\n","Epoch 5/7\n","515/516 [============================>.] - ETA: 0s - loss: 2.4627<start> i don't want to be no more <end> \n","516/516 [==============================] - 20s 39ms/step - loss: 2.4627 - val_loss: 2.5437\n","Epoch 6/7\n","516/516 [==============================] - ETA: 0s - loss: 2.4217<start> i don't want to be no more <end> \n","516/516 [==============================] - 20s 39ms/step - loss: 2.4217 - val_loss: 2.5239\n","Epoch 7/7\n","515/516 [============================>.] - ETA: 0s - loss: 2.3883<start> i don't want to be no more <end> \n","516/516 [==============================] - 20s 39ms/step - loss: 2.3881 - val_loss: 2.5077\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f29d6481750>"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.utils import validation\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=7, validation_data = dataset_test, callbacks=[CustomCallback()])"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1652789699355,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"q6aztgrAHCZ7","outputId":"4546b4be-c59b-4aeb-877e-9388e7a74a95"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"<start> i don't know what i want to be , i don't know what you want <end> \""]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model, tokenizer, init_sentence=\"<start> i\")"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1652789700097,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"5qKBnDhEeDCH","outputId":"c10cf51d-48a5-45e0-e71e-891ff3e228fb"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"<start> i love you , i don't know what i want to be your favorite girl <end> \""]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model, tokenizer, init_sentence=\"<start> i love\")"]},{"cell_type":"markdown","metadata":{"id":"5Ks60lzteK8q"},"source":["- val_loss값은 2.5077로 2.2 이하로는 내려가지 않았다. 더불어 \b시작 값을 i로 두었을 때는 같은 문장을 반복하는 모습을 보여준다.\n","- 시작 값을 i love로 두었을 때는 문맥은 조금 어색하나, 작문실력은 양호한 편인것 같다.\n","\n","------"]},{"cell_type":"markdown","metadata":{"id":"203uKQ66b2IL"},"source":["## 6-2 maxlen이 17일 때"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2495,"status":"ok","timestamp":1652789716926,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"4jn6AYeiBnlP","outputId":"f277c901-82ae-422f-a4c9-e751de96cbef"},"outputs":[{"data":{"text/plain":["[\"<start> let's stay together i , i'm i'm so in love with you <end>\",\n"," '<start> whatever you want to do <end>',\n"," '<start> is all right with me <end>',\n"," '<start> cause you make me feel so brand new <end>',\n"," \"<start> and i want to spend my life with you let me say that since , baby , since we've been together <end>\",\n"," '<start> loving you forever <end>',\n"," '<start> is what i need <end>',\n"," '<start> let me , be the one you come running to <end>',\n"," \"<start> i'll never be untrue oh baby <end>\",\n"," \"<start> let's , let's stay together gether <end>\"]"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["corpus2 = []\n","\n","for sentence2 in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence2) == 0: continue\n","    if sentence2[-1] == \":\": continue\n","    \n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence2)\n","    corpus2.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus2[:10]"]},{"cell_type":"code","execution_count":67,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3987,"status":"ok","timestamp":1652789720909,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"CCBK_1EIwKeL","outputId":"7e4d32a8-90f0-47cb-8924-75e471f5b68c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[  2 217 227 ...   0   0   0]\n"," [  2 615   7 ...   0   0   0]\n"," [  2  23  22 ...   0   0   0]\n"," ...\n"," [  2 208   1 ...   0   0   0]\n"," [  2  70   7 ...   0   0   0]\n"," [  2  14   5 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f2a1a7b8450>\n"]}],"source":["def tokenize2(corpus2):\n","    tokenizer2 = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=12000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer2.fit_on_texts(corpus2)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor2 = tokenizer2.texts_to_sequences(corpus2)   \n","\n","    tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor2, padding='post', maxlen=17, truncating = 'post')  \n","    \n","    print(tensor2,tokenizer2)\n","    return tensor2, tokenizer2\n","\n","tensor2, tokenizer2 = tokenize2(corpus2)"]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1652789739586,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"Ii3Qo60tn8BT","outputId":"69a74de1-e952-4fdb-d427-78b36cfb132e"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : the\n","6 : i\n","7 : you\n","8 : and\n","9 : to\n","10 : a\n"]}],"source":["# 단어 사전이 어떻게 구축 되어 있는지 확인\n","for idx in tokenizer2.index_word:\n","    print(idx, \":\", tokenizer2.index_word[idx])\n","\n","    if idx >= 10: break\n","#<start>가 2번 이기 때문에 모든 행의 시작은 2였다."]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1652789740678,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"qkB2UGfXof0R","outputId":"dc4c346a-8f86-4eff-d73f-615e6eba13ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["[  2 217 227 289   6   4  20  20  28  14  31  29   7   3   0   0]\n","[217 227 289   6   4  20  20  28  14  31  29   7   3   0   0   0]\n"]}],"source":["# 생성된 텐서를 소스와 타겟으로 분리\n","\n","# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input2 = tensor2[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input2 = tensor2[:, 1:]    \n","\n","print(src_input2[0])\n","print(tgt_input2[0])"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1652789741729,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"dJzvHzQBvm-k"},"outputs":[],"source":["enc_train2, enc_val2, dec_train2, dec_val2 = train_test_split(src_input2, tgt_input2, test_size = 0.3, random_state=124, shuffle=tgt_input2)"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1652789742137,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"jaiH8Z-4o2pF"},"outputs":[],"source":["BUFFER_SIZE2 = len(src_input2) # 완벽한 셔플링을 위해서는 전체 데이터 세트의 크기보다 크거나 같은 버퍼 크기가 필요\n","BATCH_SIZE2 = 256\n","steps_per_epoch2 = len(src_input2) // BATCH_SIZE2\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE2 = tokenizer2.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","\n","dataset2 = tf.data.Dataset.from_tensor_slices((src_input2, tgt_input2))\n","dataset2 = dataset2.shuffle(BUFFER_SIZE2)\n","dataset2 = dataset2.batch(BATCH_SIZE2, drop_remainder=True)"]},{"cell_type":"code","execution_count":75,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652789743051,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"EhpCU3s3o_Di"},"outputs":[],"source":["BUFFER_SIZE2 = len(enc_val2) # 완벽한 셔플링을 위해서는 전체 데이터 세트의 크기보다 크거나 같은 버퍼 크기가 필요\n","BATCH_SIZE2 = 256\n","steps_per_epoch2 = len(enc_val2) // BATCH_SIZE2\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE2 = tokenizer2.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","\n","dataset_test2 = tf.data.Dataset.from_tensor_slices((enc_val2, dec_val2))\n","dataset_test2 = dataset_test2.shuffle(BUFFER_SIZE2)\n","dataset_test2 = dataset_test2.batch(BATCH_SIZE2, drop_remainder=True)"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1652789753404,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"k0pWLsacv8xW"},"outputs":[],"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)\n","        self.rnn_3 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.2)\n","        self.linear2 = tf.keras.layers.Dense(5120)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.rnn_3(out)\n","        out = self.linear2(out)\n","        out = self.linear(out)\n","        return out\n","    \n","embedding_size = 128\n","hidden_size = 512\n","model2 = TextGenerator(tokenizer2.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1274341,"status":"ok","timestamp":1652791217355,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"AWBECQ3hwG4s","outputId":"3da99825-22bc-4c5b-943d-869a5ecc8065"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/7\n","  6/686 [..............................] - ETA: 2:40 - loss: 3.0085WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0091s vs `on_train_batch_end` time: 0.1882s). Check your callbacks.\n","686/686 [==============================] - ETA: 0s - loss: 2.8561<start> i don't want to be no more <end> \n","686/686 [==============================] - 187s 265ms/step - loss: 2.8561 - val_loss: 2.6752\n","Epoch 2/7\n","686/686 [==============================] - ETA: 0s - loss: 2.6608<start> i don't want to be no more <end> \n","686/686 [==============================] - 181s 263ms/step - loss: 2.6608 - val_loss: 2.4900\n","Epoch 3/7\n","686/686 [==============================] - ETA: 0s - loss: 2.5116<start> i don't want to be no more <end> \n","686/686 [==============================] - 181s 263ms/step - loss: 2.5116 - val_loss: 2.3335\n","Epoch 4/7\n","686/686 [==============================] - ETA: 0s - loss: 2.3854<start> i don't want to be no more <end> \n","686/686 [==============================] - 181s 263ms/step - loss: 2.3854 - val_loss: 2.1948\n","Epoch 5/7\n","686/686 [==============================] - ETA: 0s - loss: 2.2724<start> i don't want to be no more <end> \n","686/686 [==============================] - 181s 263ms/step - loss: 2.2724 - val_loss: 2.0723\n","Epoch 6/7\n","686/686 [==============================] - ETA: 0s - loss: 2.1689<start> i don't want to be no more <end> \n","686/686 [==============================] - 180s 263ms/step - loss: 2.1689 - val_loss: 1.9605\n","Epoch 7/7\n","686/686 [==============================] - ETA: 0s - loss: 2.0776<start> i don't want to be no more <end> \n","686/686 [==============================] - 184s 268ms/step - loss: 2.0776 - val_loss: 1.8603\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f29dbeaffd0>"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.utils import validation\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model2.compile(loss=loss, optimizer=optimizer)\n","model2.fit(dataset2, epochs=7, validation_data = dataset_test2, callbacks=[CustomCallback()])"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1652791225874,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"L0xGgeDXzRTj","outputId":"e3b5a158-ac08-4308-c0d4-f93eb48d7f5a"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"<start> i don't know what to do <end> \""]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model2, tokenizer2, init_sentence=\"<start> i\")"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1652791227760,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"t5VxntOx1Tkx","outputId":"ef76322a-213c-462d-9eff-ce54aa122cb3"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i love you so much <end> '"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model2, tokenizer2, init_sentence=\"<start> i love\")"]},{"cell_type":"markdown","metadata":{"id":"l2pnXe4pcQZM"},"source":["- maxlen을 17로 두었을 때에는 val_loss값이 1.8603으로 감소하였다.\n","\n","- 1번 방법과 2번 방법의 나머지 값을 동일하게 놨을 경우 문장을 제외하는 것보다는 maxlen을 사용해 끊어주는 방식이 더 val_loss를 낮출 수 있었다.\n","\n","- i와 i love를 시작 값으로 놨을 때에도 양호한 작문 실력을 보여주고 있다.\n","\n","-------"]},{"cell_type":"markdown","metadata":{"id":"xCXoWxjtgQbh"},"source":["## 6-3 maxlen이 25일 경우"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2276,"status":"ok","timestamp":1652793378517,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"_lvnCYgMV-K0","outputId":"f121e138-4dde-49a2-fcf5-30fa73271fb1"},"outputs":[{"data":{"text/plain":["[\"<start> let's stay together i , i'm i'm so in love with you <end>\",\n"," '<start> whatever you want to do <end>',\n"," '<start> is all right with me <end>',\n"," '<start> cause you make me feel so brand new <end>',\n"," \"<start> and i want to spend my life with you let me say that since , baby , since we've been together <end>\",\n"," '<start> loving you forever <end>',\n"," '<start> is what i need <end>',\n"," '<start> let me , be the one you come running to <end>',\n"," \"<start> i'll never be untrue oh baby <end>\",\n"," \"<start> let's , let's stay together gether <end>\"]"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["corpus3 = []\n","\n","for sentence3 in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence3) == 0: continue\n","    if sentence3[-1] == \":\": continue\n","    \n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence3)\n","    corpus3.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus3[:10]"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4390,"status":"ok","timestamp":1652793382902,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"3FRU0hJUWCCU","outputId":"0f52b234-6169-441d-8c12-3c841dcd2ce7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[  2 217 227 ...   0   0   0]\n"," [  2 615   7 ...   0   0   0]\n"," [  2  23  22 ...   0   0   0]\n"," ...\n"," [  2 208   1 ...   0   0   0]\n"," [  2  70   7 ...   0   0   0]\n"," [  2  14   5 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f29957c7390>\n"]}],"source":["def tokenize3(corpus3):\n","    tokenizer3 = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=12000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer3.fit_on_texts(corpus3)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor3 = tokenizer3.texts_to_sequences(corpus3)   \n","\n","    tensor3 = tf.keras.preprocessing.sequence.pad_sequences(tensor3, padding='post', maxlen=25, truncating = 'post')  \n","    \n","    print(tensor3,tokenizer3)\n","    return tensor3, tokenizer3\n","\n","tensor3, tokenizer3 = tokenize3(corpus3)"]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1652793382903,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"5haZySxoWSKt","outputId":"627afbf8-1f2a-4480-882a-d7f31a65798a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : the\n","6 : i\n","7 : you\n","8 : and\n","9 : to\n","10 : a\n"]}],"source":["# 단어 사전이 어떻게 구축 되어 있는지 확인\n","for idx in tokenizer3.index_word:\n","    print(idx, \":\", tokenizer3.index_word[idx])\n","\n","    if idx >= 10: break\n","#<start>가 2번 이기 때문에 모든 행의 시작은 2였다."]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1652793382903,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"tPREcueUWYha","outputId":"d6528a95-b3f8-4108-9890-8b474024cea7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[  2 217 227 289   6   4  20  20  28  14  31  29   7   3   0   0   0   0\n","   0   0   0   0   0   0]\n","[217 227 289   6   4  20  20  28  14  31  29   7   3   0   0   0   0   0\n","   0   0   0   0   0   0]\n"]}],"source":["# 생성된 텐서를 소스와 타겟으로 분리\n","\n","# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input3 = tensor3[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input3 = tensor3[:, 1:]    \n","\n","print(src_input3[0])\n","print(tgt_input3[0])"]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1652793382903,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"c2-mMqb2Wdoe"},"outputs":[],"source":["enc_train3, enc_val3, dec_train3, dec_val3 = train_test_split(src_input3, tgt_input3, test_size = 0.27, random_state=501, shuffle=tgt_input3)"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1652793382904,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"CPf7JRIOWnYO"},"outputs":[],"source":["BUFFER_SIZE3 = len(src_input3) # 완벽한 셔플링을 위해서는 전체 데이터 세트의 크기보다 크거나 같은 버퍼 크기가 필요\n","BATCH_SIZE3 = 128\n","steps_per_epoch3 = len(src_input3) // BATCH_SIZE3\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE3 = tokenizer3.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","dataset3 = tf.data.Dataset.from_tensor_slices((src_input3, tgt_input3))\n","dataset3 = dataset3.shuffle(BUFFER_SIZE3)\n","dataset3 = dataset3.batch(BATCH_SIZE3, drop_remainder=True)"]},{"cell_type":"code","execution_count":106,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1652793382904,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"2mI0vO6XWxbr"},"outputs":[],"source":["BUFFER_SIZE3 = len(enc_val3) # 완벽한 셔플링을 위해서는 전체 데이터 세트의 크기보다 크거나 같은 버퍼 크기가 필요\n","BATCH_SIZE3 = 128\n","steps_per_epoch3 = len(enc_val3) // BATCH_SIZE3\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE3 = tokenizer3.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","\n","dataset_test3 = tf.data.Dataset.from_tensor_slices((enc_val3, dec_val3))\n","dataset_test3 = dataset_test3.shuffle(BUFFER_SIZE3)\n","dataset_test3 = dataset_test3.batch(BATCH_SIZE3, drop_remainder=True)"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1652793382904,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"AZt8oPgXW8bU"},"outputs":[],"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.3)\n","        self.rnn_3 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout=0.2)\n","        self.linear2 = tf.keras.layers.Dense(5120)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.rnn_3(out)\n","        out = self.linear2(out)\n","        out = self.linear(out)\n","        return out\n","    \n","embedding_size = 128\n","hidden_size = 512\n","model2 = TextGenerator(tokenizer3.num_words + 1, embedding_size , hidden_size)"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401158,"status":"ok","timestamp":1652793784045,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"UdeWUh_ZXPZa","outputId":"f9f1dc1d-cd66-4b82-e14c-9e23e5744752"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","   6/1373 [..............................] - ETA: 2:04 - loss: 1.5490WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0068s vs `on_train_batch_end` time: 0.0699s). Check your callbacks.\n","1373/1373 [==============================] - ETA: 0s - loss: 1.5535<start> i know i know i know i know <end> \n","1373/1373 [==============================] - 136s 98ms/step - loss: 1.5535 - val_loss: 1.4298\n","Epoch 2/3\n","1373/1373 [==============================] - ETA: 0s - loss: 1.5418<start> i don't know what i do <end> \n","1373/1373 [==============================] - 132s 96ms/step - loss: 1.5418 - val_loss: 1.4140\n","Epoch 3/3\n","1373/1373 [==============================] - ETA: 0s - loss: 1.5320<start> i know that i was a combination of <unk> <end> \n","1373/1373 [==============================] - 133s 96ms/step - loss: 1.5320 - val_loss: 1.4045\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f290bf00950>"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["model3.compile(loss=loss, optimizer=optimizer)\n","model3.fit(dataset3, epochs=3, validation_data = dataset_test3,callbacks=[CustomCallback()])"]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":852,"status":"ok","timestamp":1652793904186,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"fM-AVkHqJs_6","outputId":"20f5bb58-e3d1-44d9-fe7b-e429836f8f2f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i know that i was a combination of <unk> <end> '"]},"execution_count":109,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model3, tokenizer3, init_sentence=\"<start> i\")"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1652793904620,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"d4uo_Kb_vjeY","outputId":"2fa534ff-4168-4927-87ab-f5907ceef260"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i love you liberian girl <end> '"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model3, tokenizer3, init_sentence=\"<start> i love\")"]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1652793908744,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"PJ7Q-sWXYsqX","outputId":"db005984-0e1b-417d-840d-43e16db94f72"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> because i was a little bit of a <unk> <end> '"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["generate_text(model3, tokenizer3, init_sentence=\"<start> because \")"]},{"cell_type":"markdown","metadata":{"id":"LLgO3ZTlgZn0"},"source":["- 3번째 모델의 epoch값은 모델 학습 시간 단축과 epoch이 늘어날 수록 작문을 함에 있어 반복적인 문장을 생성하기 때문에 값을 3으로 줄였다.  \n","- \bmaxlen의 값을 올릴 수록 val_loss가 감소하는 모습을 볼 수 있다.  \n","- 학습하지 않은 단어를 생성하려고는 하나 작문 실력역시 양호한 편을 보여준다."]},{"cell_type":"markdown","metadata":{"id":"1uIvxOzxhN0K"},"source":["----------\n","# 7. 회고\n","\n","- 여러번의 시도가 있었지만 최종적으로 가장 좋은 결과를 도출한 것은 Dense layer를 추가한 방식이었다.   \n","  해당 이유에 대해서 고민해 보자면 Dense 레이어는 추출된 정보를 하나의 일렬로 정렬해 원하는 차원으로 축소하는 개념인데  \n","  LSTM층이 끝나고 차원을 축소하는 과정에서 한 번에 많은 차원을 축소하다 보니 손실 혹은 학습에 악영향을 미치는 것 같다.  \n","  좀 더 찾아보니 Dense layer가 하나 만 있다면 조정할 가중치 값들이 너무 사라지다 보니 미세 조정이 어렵다고 한다.  \n","  딥러닝의 레이어 층에 대해서 조금씩 알가는것이 나름 재밌는 과정인것 같다.  \n","\n","- 모델을 토큰화 기준 15개로 자르는 첫 번째 과정이 maxlen을 통해 자르는 2번째 모델 보다 더 뛰어난 작문실력이 좋을거라고 예상하였다.   \n","  왜냐하면 maxlen을 통해서 문장을 자르는 것은 15개가 넘어가는 문장 중간을 자르기 때문에 문맥이 이상하게 학습이 될 것이라고 생각하였기 때문이다.  \n","  \n","  그러나 예상과는 다르게 maxlen을 활용하는 것이 작문 실력이 더 좋았으며, loss값 또한 더 낮았다.  \n","  명확한 이유는 알지 못 하였지만 간략히 생각해보면 데이터셋의 크기 차이가 원인일 수 있다고 생각이 들었다.\n","\n","- maxlen의 값을 늘리는 것이 loss값이나 작문을 하는 것에 있어서 더 좋은 성능을 보임을 알 수 있다.  \n","  해당 이유를 생각해보면 이 데이터셋 자체가 잘 형성되어 있어서 전처리를 최소화할 수록 좋은 것 같다.\n"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1652788140506,"user":{"displayName":"김건국","userId":"04251633153705627950"},"user_tz":-540},"id":"aEf7bgXmbI2k"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMqHCEkD/+PamI45WHHiY8w","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"14hbP-amuHKcprda0DFKm9yXHVBuTGZn8","name":"Exp6_Training_Git.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
